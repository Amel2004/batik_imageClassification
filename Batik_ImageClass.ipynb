{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Buka Zip"
      ],
      "metadata": {
        "id": "q7hfd18APW9A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxC-lOjlPOCB"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile('/content/MultiClass Image Classification.zip','r') as zipObj:\n",
        "  zipObj.extractall('MultiClass_Image_Classification')"
      ],
      "metadata": {
        "id": "P_MqMLvyPeY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Library yang diperlukan"
      ],
      "metadata": {
        "id": "Ms3oI3_QPjig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "wq5B5Rd5PkOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisa sederhana terhadap data"
      ],
      "metadata": {
        "id": "MfQsb188PpOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cek Jumlah data masing-masing kelas"
      ],
      "metadata": {
        "id": "9RPnFQMpPzk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_files(folder_path):\n",
        "  count = 0\n",
        "  for root, _, files in os.walk(folder_path):\n",
        "    for _ in files:\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "folder_path = \"/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-lasem\"\n",
        "file_count = count_files(folder_path)\n",
        "print(f\"Jumlah file di folder '{folder_path}': {file_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyPzJ6CyPl3H",
        "outputId": "cf81809b-c7b5-459f-9a06-e19b0e8e72e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah file di folder '/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-lasem': 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-megamendung\"\n",
        "file_count = count_files(folder_path)\n",
        "print(f\"Jumlah file di folder '{folder_path}': {file_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BBUmXqdP8Tp",
        "outputId": "67101903-1223-46c3-a403-775d715b2ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah file di folder '/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-megamendung': 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-parang\"\n",
        "file_count = count_files(folder_path)\n",
        "print(f\"Jumlah file di folder '{folder_path}': {file_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4nqH8GaP-r3",
        "outputId": "1655729c-c2f5-4968-9e73-635acd5faeb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah file di folder '/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-parang': 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Terdapat masing-masing 50 foto pada batik-parang dan batik-lasem terdapat 47 foto pada batik megamendung***\n",
        "\n",
        "Terdapat potensi overfitting karena jumlah class yang berbeda.\n",
        "\n",
        "Solusi yang kami pikrikan adalah drop dari masing-masing batik-lasem dan batik- parang.Namun sebelum itu, kami ingin tau, apakah ada foto yang corrupt."
      ],
      "metadata": {
        "id": "uyZN6CdvQDuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cek ada data yang corrupt gak"
      ],
      "metadata": {
        "id": "ZPiA6W0HQFM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "direktori = \"/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-megamendung\"\n",
        "ada_corrupt = False\n",
        "\n",
        "for filename in os.listdir(direktori):\n",
        "    filepath = os.path.join(direktori, filename)\n",
        "    try:\n",
        "\n",
        "        img = Image.open(filepath)\n",
        "        img.verify()\n",
        "    except (IOError, SyntaxError) as e:\n",
        "        print(f\"Foto '{filename}' corrupt: {e}\")\n",
        "        ada_corrupt = True\n",
        "\n",
        "if not ada_corrupt:\n",
        "    print(\"Tidak ada foto corrupt yang ditemukan di batik-megamendung\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9Tacq5_QJ69",
        "outputId": "eb5986f9-246f-4267-b20c-6ae70a5bcc58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Foto '49.jpg' corrupt: cannot identify image file '/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-megamendung/49.jpg'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "direktori = \"/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-parang\"\n",
        "ada_corrupt = False\n",
        "\n",
        "for filename in os.listdir(direktori):\n",
        "    filepath = os.path.join(direktori, filename)\n",
        "    try:\n",
        "\n",
        "        img = Image.open(filepath)\n",
        "        img.verify()\n",
        "    except (IOError, SyntaxError) as e:\n",
        "        print(f\"Foto '{filename}' corrupt: {e}\")\n",
        "        ada_corrupt = True\n",
        "\n",
        "if not ada_corrupt:\n",
        "    print(\"Tidak ada foto corrupt yang ditemukan di batik-parang\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owGDDoFCQMeb",
        "outputId": "a2d6a44b-439b-489a-f2c9-027829600c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tidak ada foto corrupt yang ditemukan di batik-parang\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "direktori = \"/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-lasem\"\n",
        "ada_corrupt = False\n",
        "\n",
        "for filename in os.listdir(direktori):\n",
        "    filepath = os.path.join(direktori, filename)\n",
        "    try:\n",
        "\n",
        "        img = Image.open(filepath)\n",
        "        img.verify()\n",
        "    except (IOError, SyntaxError) as e:\n",
        "        print(f\"Foto '{filename}' corrupt: {e}\")\n",
        "        ada_corrupt = True\n",
        "\n",
        "if not ada_corrupt:\n",
        "    print(\"Tidak ada foto corrupt yang ditemukan di batik-lasem\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awP8HclRQOMg",
        "outputId": "b7c0a8b1-4805-41bb-fcf1-a3a4d321c6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tidak ada foto corrupt yang ditemukan di batik-lasem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ternyata ditemukan 1 foto pada batik-megamendung yang corrupt, sehingga kami memutuskan untuk drop foto tersebut."
      ],
      "metadata": {
        "id": "Ua-vza0QQT5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop data yang corrupt dan cek apakah sudah kedrop atau belum"
      ],
      "metadata": {
        "id": "1JTd1ptUQXkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def remove_corrupt_images(directory):\n",
        "    for filename in os.listdir(directory):\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            try:\n",
        "\n",
        "                img = Image.open(filepath)\n",
        "\n",
        "                img.verify()\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                print(f\"Corrupt image found: {filepath}\")\n",
        "                os.remove(filepath)\n",
        "\n",
        "dataset_dir = \"/content/MultiClass_Image_Classification/MultiClass Image Classification\"\n",
        "\n",
        "for class_folder in os.listdir(dataset_dir):\n",
        "    class_path = os.path.join(dataset_dir, class_folder)\n",
        "    if os.path.isdir(class_path):\n",
        "        remove_corrupt_images(class_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1JF4kM3QQFo",
        "outputId": "2cdc7215-5706-4df7-b22d-a42b1e5d1315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrupt image found: /content/MultiClass_Image_Classification/MultiClass Image Classification/batik-megamendung/49.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_files(folder_path):\n",
        "\n",
        "  count = 0\n",
        "  for root, _, files in os.walk(folder_path):\n",
        "    for _ in files:\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "folder_path = \"/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-megamendung\"\n",
        "file_count = count_files(folder_path)\n",
        "print(f\"Jumlah file di folder '{folder_path}': {file_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZXLZWbmQecO",
        "outputId": "aae6b4eb-99bc-4f14-d5c3-446688462595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah file di folder '/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-megamendung': 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drop untuk batik-lasem dan batik-parang\n"
      ],
      "metadata": {
        "id": "au2-m-5pQg-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "original_folder_path = \"/content/MultiClass_Image_Classification/MultiClass Image Classification\"\n",
        "batik_lasem_path = os.path.join(original_folder_path, \"batik-lasem\")\n",
        "batik_parang_path = os.path.join(original_folder_path, \"batik-parang\")\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "num_images_to_drop = 4\n",
        "\n",
        "\n",
        "images_to_drop_lasem = random.sample(os.listdir(batik_lasem_path), num_images_to_drop)\n",
        "\n",
        "\n",
        "images_to_drop_parang = random.sample(os.listdir(batik_parang_path), num_images_to_drop)\n",
        "\n",
        "for image in images_to_drop_lasem:\n",
        "  image_path = os.path.join(batik_lasem_path, image)\n",
        "  os.remove(image_path)\n",
        "\n",
        "for image in images_to_drop_parang:\n",
        "  image_path = os.path.join(batik_parang_path, image)\n",
        "  os.remove(image_path)\n"
      ],
      "metadata": {
        "id": "qMCPexUxXvUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-parang\"\n",
        "file_count = count_files(folder_path)\n",
        "print(f\"Jumlah file di folder '{folder_path}': {file_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs-d4lf7aM1d",
        "outputId": "68fdcfa3-6b34-4fc3-851d-1e57703903cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah file di folder '/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-parang': 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-lasem\"\n",
        "file_count = count_files(folder_path)\n",
        "print(f\"Jumlah file di folder '{folder_path}': {file_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXlHWwsBaWbZ",
        "outputId": "3f1c122f-2c09-430f-d446-770bd158f48d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah file di folder '/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-lasem': 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cek size dan resolusi foto"
      ],
      "metadata": {
        "id": "HNUqkClea5jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder = \"/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-megamendung\"\n",
        "foto_terburuk = []\n",
        "for filename in os.listdir(folder):\n",
        "\n",
        "    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "\n",
        "        try:\n",
        "            image = Image.open(os.path.join(folder, filename))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        width, height = image.size\n",
        "\n",
        "\n",
        "        area = width * height\n",
        "\n",
        "\n",
        "        foto_terburuk.append((filename, width, height, area))\n",
        "\n",
        "\n",
        "foto_terburuk.sort(key=lambda x: x[3])\n",
        "\n",
        "print(\"5 foto dengan resolusi terburuk di batik-megamendung:\")\n",
        "for i in range(5):\n",
        "    filename, width, height, area = foto_terburuk[i]\n",
        "    print(f\"{i+1}. {filename}: {width}x{height} ({area}px)\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3523Pxj5ayax",
        "outputId": "1c3c8cb3-66f7-4fb0-ca81-81fdb9f575ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 foto dengan resolusi terburuk di batik-megamendung:\n",
            "1. 3.jpg: 236x337 (79532px)\n",
            "2. 12.jpg: 400x280 (112000px)\n",
            "3. 5.jpg: 280x400 (112000px)\n",
            "4. 26.jpg: 400x300 (120000px)\n",
            "5. 22.jpg: 400x300 (120000px)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder = \"/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-lasem\"\n",
        "foto_terburuk = []\n",
        "for filename in os.listdir(folder):\n",
        "\n",
        "    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "\n",
        "        try:\n",
        "            image = Image.open(os.path.join(folder, filename))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        width, height = image.size\n",
        "\n",
        "\n",
        "        area = width * height\n",
        "\n",
        "\n",
        "        foto_terburuk.append((filename, width, height, area))\n",
        "\n",
        "\n",
        "foto_terburuk.sort(key=lambda x: x[3])\n",
        "\n",
        "print(\"5 foto dengan resolusi terburuk di batik-lasem:\")\n",
        "for i in range(5):\n",
        "    filename, width, height, area = foto_terburuk[i]\n",
        "    print(f\"{i+1}. {filename}: {width}x{height} ({area}px)\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gf7W0eabi0f",
        "outputId": "60bd30b9-d76c-4209-fc0b-3ddaad129718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 foto dengan resolusi terburuk di batik-lasem:\n",
            "1. 37.jpg: 200x113 (22600px)\n",
            "2. 11.jpg: 300x216 (64800px)\n",
            "3. 12.jpg: 300x225 (67500px)\n",
            "4. 4.jpg: 320x212 (67840px)\n",
            "5. 17.jpg: 361x288 (103968px)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder = \"/content/MultiClass_Image_Classification/MultiClass Image Classification/batik-parang\"\n",
        "foto_terburuk = []\n",
        "for filename in os.listdir(folder):\n",
        "\n",
        "    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "\n",
        "        try:\n",
        "            image = Image.open(os.path.join(folder, filename))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        width, height = image.size\n",
        "\n",
        "\n",
        "        area = width * height\n",
        "\n",
        "\n",
        "        foto_terburuk.append((filename, width, height, area))\n",
        "\n",
        "\n",
        "foto_terburuk.sort(key=lambda x: x[3])\n",
        "\n",
        "print(\"5 foto dengan resolusi terburuk di batik-parang:\")\n",
        "for i in range(5):\n",
        "    filename, width, height, area = foto_terburuk[i]\n",
        "    print(f\"{i+1}. {filename}: {width}x{height} ({area}px)\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQzx9Cnbbtyn",
        "outputId": "5f64645c-a5f4-49ec-f126-861c56c906d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 foto dengan resolusi terburuk di batik-parang:\n",
            "1. 50.jpg: 229x220 (50380px)\n",
            "2. 30.jpg: 305x218 (66490px)\n",
            "3. 26.jpg: 300x225 (67500px)\n",
            "4. 5.jpg: 310x220 (68200px)\n",
            "5. 2.jpg: 300x232 (69600px)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dari hasil tersebut kita bisa tau bahwa ukuran-ukuran foto pada data kita berbeda-beda."
      ],
      "metadata": {
        "id": "P0GTCbUJb2Z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data prep & Model"
      ],
      "metadata": {
        "id": "VUHj5CfucXcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE=224\n",
        "BATCH_SIZE=64"
      ],
      "metadata": {
        "id": "6oB7UuBultRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/MultiClass_Image_Classification/MultiClass Image Classification\""
      ],
      "metadata": {
        "id": "x5tGon9_pmQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen=tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.1\n",
        "    )\n",
        "\n",
        "test_datagen=tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "     rescale=1./255,\n",
        "     validation_split=0.1\n",
        ")\n",
        "\n",
        "train_datagen=train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "test_datagen=test_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYVK3c19puMf",
        "outputId": "418e50a2-8938-4355-ce69-dc7cda64a1eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 126 images belonging to 3 classes.\n",
            "Found 12 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1"
      ],
      "metadata": {
        "id": "8oAHVDV-0yjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn=tf.keras.Sequential()\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=64,padding='same',strides=2,kernel_size=3,activation='relu',input_shape=(224,224,3)))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32,padding='same',strides=2,kernel_size=3,activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32,padding='same',strides=2,kernel_size=3,activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2))\n",
        "\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "cnn.add(tf.keras.layers.Dense(3,activation='softmax'))"
      ],
      "metadata": {
        "id": "zZ8s_8Z3qs5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "print(cnn.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqf69OQDt_aS",
        "outputId": "908ec44e-82a6-4c83-df19-26a85fb14711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 112, 112, 64)      1792      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 56, 56, 64)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 32)        18464     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 14, 14, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 32)          9248      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 3, 3, 32)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 288)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 867       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30371 (118.64 KB)\n",
            "Trainable params: 30371 (118.64 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer=tf.keras.optimizers.Adam(),loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "68cK0bGKvT9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "cnn.fit(train_datagen, epochs=20, validation_data=test_datagen, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIj-C2_kvyR9",
        "outputId": "82160d9e-373e-4fef-de18-3eae0248c5c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2/2 [==============================] - 7s 3s/step - loss: 0.6710 - accuracy: 0.7698 - val_loss: 0.4810 - val_accuracy: 0.8333\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6685 - accuracy: 0.7619 - val_loss: 0.4535 - val_accuracy: 0.9167\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 6s 3s/step - loss: 0.6282 - accuracy: 0.7460 - val_loss: 0.4262 - val_accuracy: 0.8333\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6372 - accuracy: 0.7937 - val_loss: 0.4220 - val_accuracy: 0.8333\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 7s 3s/step - loss: 0.6280 - accuracy: 0.7857 - val_loss: 0.4238 - val_accuracy: 0.9167\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6005 - accuracy: 0.8175 - val_loss: 0.4057 - val_accuracy: 0.9167\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 6s 3s/step - loss: 0.5772 - accuracy: 0.7937 - val_loss: 0.3847 - val_accuracy: 0.8333\n",
            "Epoch 7: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cbb54716bc0>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 5"
      ],
      "metadata": {
        "id": "4WIk5qwG3btY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = tf.keras.Sequential()\n",
        "\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', input_shape=(224, 224, 3)))  # Increased filter size, adjusted stride\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=3, padding='same'))\n",
        "cnn.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=3, padding='same', activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='same'))\n",
        "cnn.add(tf.keras.layers.BatchNormalization())\n",
        "cnn.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=3, padding='same'))\n",
        "cnn.add(tf.keras.layers.BatchNormalization())\n",
        "cnn.add(tf.keras.layers.Dropout(0.4))\n",
        "\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "cnn.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
        "cnn.add(tf.keras.layers.Dense(3, activation='softmax'))"
      ],
      "metadata": {
        "id": "qmfclMPO3d1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer=tf.keras.optimizers.Adam(),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "cnn.fit(train_datagen, epochs=20, validation_data=test_datagen, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zEprt_7COfZ",
        "outputId": "ac053cb7-0571-4bd2-97fd-c0841ae0db97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2/2 [==============================] - 7s 2s/step - loss: 0.8217 - accuracy: 0.7063 - val_loss: 1.0411 - val_accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 7s 4s/step - loss: 0.5546 - accuracy: 0.8175 - val_loss: 1.0261 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.3797 - accuracy: 0.8175 - val_loss: 1.0004 - val_accuracy: 0.3333\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5164 - accuracy: 0.7857 - val_loss: 0.9972 - val_accuracy: 0.4167\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 6s 2s/step - loss: 0.4031 - accuracy: 0.8571 - val_loss: 1.0419 - val_accuracy: 0.3333\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.3672 - accuracy: 0.8889 - val_loss: 0.9989 - val_accuracy: 0.3333\n",
            "Epoch 6: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fc9b7a25b10>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 6"
      ],
      "metadata": {
        "id": "4IfSBqYu4Nki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = tf.keras.Sequential()\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', input_shape=(224, 224, 3)))  # Increased filter size, adjusted stride\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=3, padding='same'))\n",
        "cnn.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=3, padding='same', activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=3, padding='same'))\n",
        "cnn.add(tf.keras.layers.BatchNormalization())\n",
        "cnn.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=3, padding='same'))\n",
        "cnn.add(tf.keras.layers.BatchNormalization())\n",
        "cnn.add(tf.keras.layers.Dropout(0.4))\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=512, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='same'))\n",
        "cnn.add(tf.keras.layers.BatchNormalization())\n",
        "cnn.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "cnn.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
        "cnn.add(tf.keras.layers.Dense(3, activation='softmax'))"
      ],
      "metadata": {
        "id": "WA7ofqKX4jBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer=tf.keras.optimizers.Adam(),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "cnn.fit(train_datagen, epochs=20, validation_data=test_datagen, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBghHWcFF5cl",
        "outputId": "a7d88466-9b87-4c51-da58-ebd6faa8491b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2/2 [==============================] - 9s 2s/step - loss: 0.6901 - accuracy: 0.7698 - val_loss: 1.0694 - val_accuracy: 0.5833\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 6s 2s/step - loss: 0.6565 - accuracy: 0.7460 - val_loss: 1.0676 - val_accuracy: 0.4167\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 6s 2s/step - loss: 0.3884 - accuracy: 0.8333 - val_loss: 1.0617 - val_accuracy: 0.3333\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6189 - accuracy: 0.7857 - val_loss: 1.0559 - val_accuracy: 0.3333\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 7s 3s/step - loss: 0.4687 - accuracy: 0.7937 - val_loss: 1.0493 - val_accuracy: 0.6667\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.2687 - accuracy: 0.8968 - val_loss: 1.0549 - val_accuracy: 0.5833\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 6s 3s/step - loss: 0.6182 - accuracy: 0.7937 - val_loss: 1.0702 - val_accuracy: 0.4167\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.3595 - accuracy: 0.8810 - val_loss: 1.0874 - val_accuracy: 0.3333\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 7s 3s/step - loss: 0.4258 - accuracy: 0.8413 - val_loss: 1.0869 - val_accuracy: 0.3333\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.3498 - accuracy: 0.8889 - val_loss: 1.0885 - val_accuracy: 0.3333\n",
            "Epoch 10: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fc9b7a25d80>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 7\n"
      ],
      "metadata": {
        "id": "xJHIVfOq5nXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = tf.keras.Sequential()\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', input_shape=(224, 224, 3)))  # Increased filter size, adjusted stride\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=3, padding='same'))\n",
        "cnn.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=3, padding='same', activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=3, padding='same'))\n",
        "cnn.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=4, strides=4, padding='same'))\n",
        "cnn.add(tf.keras.layers.BatchNormalization())\n",
        "cnn.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=512, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=4, strides=4, padding='same'))\n",
        "cnn.add(tf.keras.layers.BatchNormalization())\n",
        "cnn.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "cnn.add(tf.keras.layers.Dense(1024, activation='relu'))"
      ],
      "metadata": {
        "id": "jVqHRP975pP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer=tf.keras.optimizers.Adam(),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "cnn.fit(train_datagen, epochs=20, validation_data=test_datagen, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2_XAAweHTsH",
        "outputId": "4ad273e3-880c-4054-e66d-c284d3e0615b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2/2 [==============================] - 8s 3s/step - loss: 1.2766 - accuracy: 0.4603 - val_loss: 1.0849 - val_accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6644 - accuracy: 0.7778 - val_loss: 1.0803 - val_accuracy: 0.3333\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6874 - accuracy: 0.7302 - val_loss: 1.0752 - val_accuracy: 0.3333\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 6s 3s/step - loss: 0.7533 - accuracy: 0.6825 - val_loss: 1.0680 - val_accuracy: 0.3333\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4691 - accuracy: 0.8175 - val_loss: 1.0621 - val_accuracy: 0.5833\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.3775 - accuracy: 0.8254 - val_loss: 1.0574 - val_accuracy: 0.6667\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 6s 2s/step - loss: 0.4568 - accuracy: 0.8254 - val_loss: 1.0508 - val_accuracy: 0.4167\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4799 - accuracy: 0.8333 - val_loss: 1.0421 - val_accuracy: 0.6667\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 5s 3s/step - loss: 0.3570 - accuracy: 0.8571 - val_loss: 1.0332 - val_accuracy: 0.5833\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1830 - accuracy: 0.9286 - val_loss: 1.0230 - val_accuracy: 0.4167\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 6s 3s/step - loss: 0.2652 - accuracy: 0.8889 - val_loss: 1.0310 - val_accuracy: 0.5000\n",
            "Epoch 11: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fc97aca8d90>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    }
  ]
}